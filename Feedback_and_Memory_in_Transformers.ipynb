{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Feedback and Memory in Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajaswa/feedback-and-memory-in-transformers/blob/main/Feedback_and_Memory_in_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOKvrKmnjpdR"
      },
      "source": [
        "This notebook explores the role of Feedback and Memory in Transformer models. The notebook is based on this paper:\n",
        "\n",
        "*Fan, A., Lavril, T., Grave, E., Joulin, A., & Sukhbaatar, S. (2020). [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402). arXiv preprint arXiv:2002.09402.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wdRGK9PU6MM"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07leXHx2UzmG"
      },
      "source": [
        "# Check GPU Allotment\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDICNw5gY80Q"
      },
      "source": [
        "# Install Dependencies\n",
        "!pip -q install pytorch-lightning\n",
        "!pip -q install torchmetrics"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ_YZTiBVP9V"
      },
      "source": [
        "# IMPORTS\n",
        "\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torchmetrics\n",
        "\n",
        "# SEED\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lu7lg7WYT_Y"
      },
      "source": [
        "# Understanding the Importance of Feedback and Memory in Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSldJP2Z-9qc"
      },
      "source": [
        "### Top-down vs Bottom-up Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXbmHNfbk3nb"
      },
      "source": [
        "Most of the Deep Neural Network models follow a **Bottom-up processing** approach, where given a stimulus, high-level latent abstract representations are obtained. These representations are then eventually used for some downstream task.\n",
        "\n",
        "On the other hand, **Top-down processing** relies on world-knowledge and previosuly known facts & beliefs in the memory. Feedback and Memory play a very important role in Top-down processing. Given below is a short video by **Khan Academy** explaining the difference between **Bottom-up & Top-down processing**, and their individual importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "M7pG1j8HVGDa",
        "outputId": "21eb6233-e1ce-4ba1-e040-ed4181ac2b96"
      },
      "source": [
        "# Understanding the Role of Feedback and Top-down processing in Cognition & Perception\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aJy5_p_LAhQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aJy5_p_LAhQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDV8MKD0-3YL"
      },
      "source": [
        "### Limitations of Transformers in Sequential Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xnOv0s0DZf2"
      },
      "source": [
        "[Transformer models](http://jalammar.github.io/illustrated-transformer/) are quite scalable due to their parallel processing capabilities. They handle sequential processing by employing a self-attention mechanism over the entire input sequence. While this helps in capturing a (bi-directional) sequential context while processing the input at a given time-step, it blocks the access to high-level abstract representations from the past time-steps. While this still allows the normal Bottom-up processing, there's no scope for Top-down processing. This results in two major limitations of transformers in sequential tasks:\n",
        "\n",
        "\n",
        "1.   **Limited Access to Higher Level Representations:** This allows the transformer model to perform only a limited number of state-updates to the input states.\n",
        "2.   **Maintaining a Belief State:** This doesn't allow the transformer to work with longer sequences, which requires a good memory component.\n",
        "\n",
        "\n",
        "We'll discuss both these limitations in detail by conducting experiments with a representative task for each of them. We'll also probe how introducding a Top-down feedback with memory helps tackle these limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4fC_d6LRy4Z"
      },
      "source": [
        "# Feedback Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU8WxenXUHue"
      },
      "source": [
        "![Feedback Transformer](https://raw.githubusercontent.com/rajaswa/feedback-and-memory-in-transformers/main/figures/feedback_transformer.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqfRokY8Uyy8"
      },
      "source": [
        "**Feedback Transformer** addresses both the above mentioned limitations of **Vanilla Transformer** by performing **Sequential Processing**, instead of the usual Parallel Processing of input sequence. This is done by simply changing the **attention-mechanism** in the architecture:\n",
        "\n",
        "1.   ***Self-Attention over Outputs from Previous Layers is Discarded***\n",
        "2.   ***Attention is instead employed over the Memory states from past input-steps***\n",
        "3.   ***Where, Memory states are obtained by a learnable weighted-summation of all the hidden representations for the particular time-step.***\n",
        "\n",
        "This, enables the access to high-level representations across all the layers. This also allows feedback, where a sub-layer can feed itself via memory. While this is essentially a sequential model, it has **key differences with Recurrent Architectures like multi-layered RNNs and LSTMs**. Each layer of these models has recurrent connections to the same layer, but not to higher layers. Morever,  their internal state has a limited capacity determined by the number of layers and their hidden dimension. In contrast, the internal state of a Feedback Transformer is its whole memory, which can grow with the input length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtnLyDRljBgk"
      },
      "source": [
        "\"\"\"\n",
        "FEEDBACK TRANSFORMER UTILITIES\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=400):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class LinearWeightedAvg(nn.Module):\n",
        "    def __init__(self, n_inputs):\n",
        "        super(LinearWeightedAvg, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(n_inputs))\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        res = 0\n",
        "        weights = self.softmax(self.weights)\n",
        "        for emb_idx, emb in enumerate(input):\n",
        "            res += emb * weights[emb_idx]\n",
        "        return res\n",
        "\n",
        "\n",
        "class FeedforwardBlock(nn.Module):\n",
        "    def __init__(self, d_model, dim_feedforward=2048, dropout=0.1, activation=\"gelu\"):\n",
        "        super(FeedforwardBlock, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout_projection = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.dropout_residual = nn.Dropout(dropout)\n",
        "        self.norm_ff = nn.LayerNorm(d_model)\n",
        "        self.dropout_ff = nn.Dropout(dropout)\n",
        "        self.activation = self._get_activation_fn(activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden_state = self.dropout_projection(\n",
        "            self.activation(self.linear1(x))\n",
        "        )  # projection\n",
        "        ff_output = self.dropout_ff(self.linear2(hidden_state))  # feed-forward\n",
        "        output = x + self.dropout_residual(ff_output)  # residual-connection\n",
        "        output = self.norm_ff(output)\n",
        "        return output\n",
        "\n",
        "    def _get_activation_fn(self, activation):\n",
        "        if activation == \"relu\":\n",
        "            return F.relu\n",
        "        elif activation == \"gelu\":\n",
        "            return F.gelu\n",
        "        raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB63stEIov9X"
      },
      "source": [
        "\"\"\"\n",
        "FEEDBACK TRANSFORMER ENCODER\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FeedbackTransformerPointwiseEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"gelu\",\n",
        "    ):\n",
        "        super(FeedbackTransformerPointwiseEncoder, self).__init__()\n",
        "\n",
        "        # memory-attention\n",
        "        self.memory_layer_wise_weighting = LinearWeightedAvg(n_inputs=num_layers)\n",
        "        self.mem_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.mem_attn_dropout = nn.Dropout(dropout)\n",
        "        self.mem_attn_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # feedforward\n",
        "        self.feedforward_layers = self._get_clones(\n",
        "            FeedforwardBlock(d_model, dim_feedforward, dropout, activation), num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, src_embed, memory_states):\n",
        "        # layer-wise memory-attention and feedforward\n",
        "        layer_wise_outputs = []\n",
        "        memory_states = torch.stack(memory_states)\n",
        "        output_embed = src_embed\n",
        "\n",
        "        for feedforward in self.feedforward_layers:\n",
        "\n",
        "            # memory-attention\n",
        "            mem_attn_out, _ = self.mem_attn(\n",
        "                query=output_embed, key=memory_states, value=memory_states\n",
        "            )\n",
        "            output_embed = output_embed + self.mem_attn_dropout(mem_attn_out)\n",
        "            output_embed = self.mem_attn_norm(output_embed)\n",
        "\n",
        "            # feedforward\n",
        "            output_embed = feedforward(output_embed)\n",
        "            layer_wise_outputs.append(output_embed)\n",
        "\n",
        "        # output memory-state for current time-step\n",
        "        output_memory_state = self.memory_layer_wise_weighting(layer_wise_outputs)\n",
        "        return output_embed, output_memory_state\n",
        "\n",
        "    def _get_clones(self, module, N):\n",
        "        return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "class FeedbackTransformerEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        memory_context=16,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"gelu\",\n",
        "    ):\n",
        "        super(FeedbackTransformerEncoder, self).__init__()\n",
        "\n",
        "        self.memory_context = memory_context\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.pointwise_encoder = FeedbackTransformerPointwiseEncoder(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, mask, src_key_padding_mask):\n",
        "        # memory context\n",
        "        bs = src.shape[1]\n",
        "        memory_states = [\n",
        "            torch.zeros(bs, self.d_model).to(src.device)\n",
        "            for i in range(self.memory_context)\n",
        "        ]\n",
        "\n",
        "        # iterate over entire sequence-length\n",
        "        pred_seq_logits = []\n",
        "        for i in range(src.shape[0]):\n",
        "            output_embed, output_memory_state = self.pointwise_encoder(\n",
        "                torch.unsqueeze(src[i], dim=0), memory_states\n",
        "            )\n",
        "            pred_seq_logits.append(torch.squeeze(output_embed))\n",
        "            memory_states = [torch.squeeze(output_memory_state)] + memory_states[:-1]\n",
        "        pred_seq_logits = self.norm(torch.stack(pred_seq_logits))\n",
        "        return pred_seq_logits"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qXhrpMZjP9Z"
      },
      "source": [
        "\"\"\"\n",
        "FEEDBACK TRANSFORMER DECODER\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FeedbackTransformerPointwiseDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"gelu\",\n",
        "    ):\n",
        "        super(FeedbackTransformerPointwiseDecoder, self).__init__()\n",
        "\n",
        "        # cross-attention encoder-decoder\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.cross_dropout = nn.Dropout(dropout)\n",
        "        self.cross_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # memory-attention\n",
        "        self.memory_layer_wise_weighting = LinearWeightedAvg(n_inputs=num_layers)\n",
        "        self.mem_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.mem_attn_dropout = nn.Dropout(dropout)\n",
        "        self.mem_attn_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # feedforward\n",
        "        self.feedforward_layers = self._get_clones(\n",
        "            FeedforwardBlock(d_model, dim_feedforward, dropout, activation), num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, tgt_embed, memory_states, encoder_outputs):\n",
        "        # layer-wise memory-attention and feedforward\n",
        "        layer_wise_outputs = []\n",
        "        memory_states = torch.stack(memory_states)\n",
        "        output_embed = tgt_embed\n",
        "\n",
        "        for feedforward in self.feedforward_layers:\n",
        "\n",
        "            # memory-attention\n",
        "            mem_attn_out, _ = self.mem_attn(\n",
        "                query=output_embed, key=memory_states, value=memory_states\n",
        "            )\n",
        "            output_embed = output_embed + self.mem_attn_dropout(mem_attn_out)\n",
        "            output_embed = self.mem_attn_norm(output_embed)\n",
        "\n",
        "            # cross-attention to encoder outputs\n",
        "            output_embed2, _ = self.cross_attn(\n",
        "                output_embed, encoder_outputs, encoder_outputs\n",
        "            )\n",
        "            output_embed = output_embed + self.cross_dropout(output_embed2)\n",
        "            output_embed = self.cross_norm(output_embed)\n",
        "\n",
        "            # feedforward\n",
        "            output_embed = feedforward(output_embed)\n",
        "            layer_wise_outputs.append(output_embed)\n",
        "\n",
        "        # output memory-state for current time-step\n",
        "        output_memory_state = self.memory_layer_wise_weighting(layer_wise_outputs)\n",
        "        return output_embed, output_memory_state\n",
        "\n",
        "    def _get_clones(self, module, N):\n",
        "        return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "class FeedbackTransformerDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        memory_context=16,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_layers=2,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"gelu\",\n",
        "    ):\n",
        "        super(FeedbackTransformerDecoder, self).__init__()\n",
        "\n",
        "        self.memory_context = memory_context\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.pointwise_decoder = FeedbackTransformerPointwiseDecoder(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt,\n",
        "        encoder_outputs,\n",
        "        tgt_mask,\n",
        "        memory_mask,\n",
        "        tgt_key_padding_mask,\n",
        "        memory_key_padding_mask,\n",
        "    ):\n",
        "        # memory context\n",
        "        bs = tgt.shape[1]\n",
        "        memory_states = [\n",
        "            torch.zeros(bs, self.d_model).to(tgt.device)\n",
        "            for i in range(self.memory_context)\n",
        "        ]\n",
        "\n",
        "        # iterate over entire sequence-length\n",
        "        pred_seq_logits = []\n",
        "        for i in range(tgt.shape[0]):\n",
        "            output_embed, output_memory_state = self.pointwise_decoder(\n",
        "                torch.unsqueeze(tgt[i], dim=0), memory_states, encoder_outputs\n",
        "            )\n",
        "            pred_seq_logits.append(torch.squeeze(output_embed))\n",
        "            memory_states = [torch.squeeze(output_memory_state)] + memory_states[:-1]\n",
        "        pred_seq_logits = self.norm(torch.stack(pred_seq_logits))\n",
        "        return pred_seq_logits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RP9BHXji93e"
      },
      "source": [
        "\"\"\"\n",
        "END TO END FEEDBACK TRANSFORMER MODEL\n",
        "\"\"\"\n",
        "\n",
        "class FeedbackTransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_feedback=False, decoder_feedback=True, memory_context=16, vocab_size=11, d_model=256, nhead=8, num_layers=4, dim_feedforward=2048, dropout=0.1, max_seq_length=400, PAD_IDX=10, activation=\"gelu\"):\n",
        "        super(FeedbackTransformerModel, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.PAD_IDX = PAD_IDX\n",
        "\n",
        "        # Embeddings\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model, dropout=dropout, max_len=max_seq_length\n",
        "        )\n",
        "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Feedback Transformer\n",
        "        if encoder_feedback:\n",
        "            feedback_encoder = FeedbackTransformerEncoder(\n",
        "                memory_context=memory_context,\n",
        "                d_model=d_model,\n",
        "                nhead=nhead,\n",
        "                num_layers=num_layers,\n",
        "                dim_feedforward=dim_feedforward,\n",
        "                dropout=dropout,\n",
        "                activation=activation)\n",
        "        else:\n",
        "            feedback_encoder = None\n",
        "            \n",
        "        if decoder_feedback:\n",
        "            feedback_decoder = FeedbackTransformerDecoder(\n",
        "                memory_context=memory_context,\n",
        "                d_model=d_model,\n",
        "                nhead=nhead,\n",
        "                num_layers=num_layers,\n",
        "                dim_feedforward=dim_feedforward,\n",
        "                dropout=dropout,\n",
        "                activation=activation)\n",
        "        else:\n",
        "            feedback_decoder = None\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            custom_encoder=feedback_encoder,\n",
        "            custom_decoder=feedback_decoder,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "        )\n",
        "\n",
        "        self.lm_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_seq, output_seq, flatten_lm_output=False):\n",
        "        # Input Sequence (N,S) -> Permuted Input Sequence (S,N)\n",
        "        input_seq = input_seq.permute(1, 0)\n",
        "        output_seq = output_seq.permute(1, 0)\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(input_seq, output_seq, self.PAD_IDX)\n",
        "\n",
        "        src_embeddings = self.pos_encoder(\n",
        "            self.src_embedding(input_seq) * math.sqrt(self.d_model)\n",
        "        )\n",
        "        tgt_embeddings = self.pos_encoder(\n",
        "            self.tgt_embedding(input_seq) * math.sqrt(self.d_model)\n",
        "        )\n",
        "        \n",
        "        transformer_outputs = self.transformer(\n",
        "            src=src_embeddings,\n",
        "            tgt=tgt_embeddings,\n",
        "            src_mask=src_mask.to(src_embeddings.device),\n",
        "            tgt_mask=tgt_mask.to(tgt_embeddings.device),\n",
        "            src_key_padding_mask=src_padding_mask.to(src_embeddings.device), \n",
        "            tgt_key_padding_mask=tgt_padding_mask.to(tgt_embeddings.device),\n",
        "        )\n",
        "\n",
        "        pred_seq_logits = self.lm_layer(transformer_outputs).permute(1, 0, 2)\n",
        "        if flatten_lm_output:\n",
        "            pred_seq_logits = pred_seq_logits.reshape(-1, self.vocab_size)\n",
        "        return pred_seq_logits\n",
        "    \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def create_mask(self, src, tgt, PAD_IDX):\n",
        "        src_seq_len = src.shape[0]\n",
        "        tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
        "        src_mask = torch.zeros((src_seq_len, src_seq_len)).type(torch.bool)\n",
        "\n",
        "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "        tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "\n",
        "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daIEc4GAya6P"
      },
      "source": [
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# dataset = SequenceCopyDataset()\n",
        "# dataloader = DataLoader(dataset, batch_size=64)\n",
        "\n",
        "# model = FeedbackTransformerModel().to(\"cuda\")\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# for i, batch in enumerate(dataloader):\n",
        "#     input_seq, output_seq = batch\n",
        "#     optimizer.zero_grad()\n",
        "#     outputs = model(input_seq.to(\"cuda\"), output_seq.to(\"cuda\"), flatten_lm_output=True)\n",
        "#     print(outputs.shape)\n",
        "#     loss = F.cross_entropy(outputs, output_seq.view(-1).to(\"cuda\"))\n",
        "#     # print(loss)\n",
        "#     loss.backward()\n",
        "\n",
        "#     optimizer.step()\n",
        "#     # for name, param in model.named_parameters():\n",
        "#     #   if param.grad == None:\n",
        "#     #       print(name)\n",
        "#     # print(outputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an71fKAw6td9"
      },
      "source": [
        "# Sequence Copy & Reverse Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_z7T0wW-8LW"
      },
      "source": [
        "# PYTORCH DATASET\n",
        "\n",
        "\n",
        "class SequenceCopyDataset(Dataset):\n",
        "    def __init__(self, num_samples=10000, max_length=40, reverse=True):\n",
        "        super().__init__()\n",
        "        self.sequence_pairs = self.generate_samples(num_samples, max_length, reverse)\n",
        "\n",
        "    def generate_samples(self, num_samples=1000, max_length=40, reverse=False):\n",
        "        sequence_pairs = []\n",
        "        for i in range(num_samples):\n",
        "            input_sequence = [\n",
        "                random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) for _ in range(max_length)\n",
        "            ]\n",
        "            if reverse:\n",
        "                output_sequence = [\n",
        "                    input_sequence[-1 * i] for i in range(1, max_length + 1)\n",
        "                ]\n",
        "            else:\n",
        "                output_sequence = input_sequence.copy()\n",
        "            sequence_pairs.append({\"input\": input_sequence, \"output\": output_sequence})\n",
        "        return sequence_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequence_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        sample = (\n",
        "            torch.tensor(self.sequence_pairs[idx][\"input\"]),\n",
        "            torch.tensor(self.sequence_pairs[idx][\"output\"]),\n",
        "        )\n",
        "        return sample"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXSL1CHPVJ86"
      },
      "source": [
        "# LIGHTNING DATAMODULE\n",
        "\n",
        "\n",
        "class SequenceCopyDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size=32,\n",
        "        num_workers=4,\n",
        "        num_samples_train=10000,\n",
        "        num_samples_eval=2000,\n",
        "        max_length_train=40,\n",
        "        max_length_eval=60,\n",
        "        reverse=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_samples_train = num_samples_train\n",
        "        self.num_samples_eval = num_samples_eval\n",
        "        self.max_length_train = max_length_train\n",
        "        self.max_length_eval = max_length_eval\n",
        "        self.reverse = reverse\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_dataset = SequenceCopyDataset(\n",
        "                num_samples=self.num_samples_train,\n",
        "                max_length=self.max_length_train,\n",
        "                reverse=self.reverse,\n",
        "            )\n",
        "            self.valid_dataset = SequenceCopyDataset(\n",
        "                num_samples=self.num_samples_eval,\n",
        "                max_length=self.max_length_eval,\n",
        "                reverse=self.reverse,\n",
        "            )\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_dataset = SequenceCopyDataset(\n",
        "                num_samples=self.num_samples_eval,\n",
        "                max_length=self.max_length_eval,\n",
        "                reverse=self.reverse,\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.valid_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            drop_last=True,\n",
        "        )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLseYTzXLTWx"
      },
      "source": [
        "# LIGHTNING MODULE\n",
        "\n",
        "\n",
        "class SequenceCopyModel(pl.LightningModule):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.accuracy = torchmetrics.Accuracy()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_seq, output_seq = batch\n",
        "        pred_seq_logits = self.model(input_seq, output_seq, flatten_lm_output=True)\n",
        "        loss = self.loss(pred_seq_logits, output_seq.view(-1))\n",
        "\n",
        "        self.log(\n",
        "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_seq, output_seq = batch\n",
        "        pred_seq_logits = self.model(input_seq, output_seq, flatten_lm_output=True)\n",
        "        loss = self.loss(pred_seq_logits, output_seq.view(-1))\n",
        "\n",
        "        self.log(\n",
        "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "        return {\"loss\": loss, \"pred\": pred_seq_logits, \"ground_truth\": output_seq.view(-1)}\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        preds, ground_truths = [], []\n",
        "        for out in validation_step_outputs:\n",
        "            preds.append(torch.argmax(out[\"pred\"], dim=1).tolist())\n",
        "            ground_truths.append(out[\"ground_truth\"].tolist())\n",
        "        accuracy = self.accuracy(torch.tensor(preds), torch.tensor(ground_truths))\n",
        "\n",
        "        self.log(\n",
        "            \"epoch_val_accuracy\", accuracy, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_seq, output_seq = batch\n",
        "        pred_seq_logits = self.model(input_seq, output_seq, flatten_lm_output=True)\n",
        "        loss = self.loss(pred_seq_logits, output_seq.view(-1))\n",
        "\n",
        "        self.log(\n",
        "            \"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "        return {\"loss\": loss, \"pred\": pred_seq_logits, \"ground_truth\": output_seq.view(-1)}\n",
        "\n",
        "    def test_epoch_end(self, test_step_outputs):\n",
        "        preds, ground_truths = [], []\n",
        "        for out in test_step_outputs:\n",
        "            preds.append(torch.argmax(out[\"pred\"], dim=1).tolist())\n",
        "            ground_truths.append(out[\"ground_truth\"].tolist())\n",
        "        accuracy = self.accuracy(torch.tensor(preds), torch.tensor(ground_truths))\n",
        "\n",
        "        self.log(\n",
        "            \"epoch_test_accuracy\", accuracy, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-3, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode=\"min\", factor=0.5, patience=1\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler,\n",
        "            \"monitor\": \"epoch_val_accuracy\",\n",
        "        }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K22N-pcQJBMC"
      },
      "source": [
        "# TRAINING SETUP\n",
        "\n",
        "trainer_flags = {\n",
        "    \"amp_backend\": \"native\",\n",
        "    \"benchmark\": False,\n",
        "    \"deterministic\": False,\n",
        "    \"callbacks\": [\n",
        "        ModelCheckpoint(monitor=\"epoch_val_accuracy\"),\n",
        "        EarlyStopping(monitor=\"epoch_val_accuracy\", mode=\"max\", patience=3),\n",
        "    ],\n",
        "    \"gpus\": 1,\n",
        "    \"log_every_n_steps\": 10,\n",
        "    \"logger\": TensorBoardLogger(save_dir=\"logs/\", name=\"sequence_copy_reverse_logs\"),\n",
        "    \"max_epochs\": 100,\n",
        "    \"progress_bar_refresh_rate\": 20,\n",
        "}\n",
        "\n",
        "# TRAINING DATA SETUP\n",
        "datamodule = SequenceCopyDataModule(\n",
        "    batch_size=64,\n",
        "    num_workers=2,\n",
        "    num_samples_train=20000,\n",
        "    num_samples_eval=8000,\n",
        "    max_length_train=40,\n",
        "    max_length_eval=100,\n",
        "    reverse=True,\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXrtJ0m-iKF7"
      },
      "source": [
        "# TRAIN FEEDBACK-TRANSFORMER SEQ2SEQ MODEL\n",
        "\n",
        "model = SequenceCopyModel(\n",
        "    model=FeedbackTransformerModel(\n",
        "        encoder_feedback=False,\n",
        "        decoder_feedback=False,\n",
        "        memory_context=8,\n",
        "        vocab_size=11,\n",
        "        d_model=64,\n",
        "        nhead=8,\n",
        "        num_layers=4,\n",
        "        dim_feedforward=128,\n",
        "        max_seq_length=100,\n",
        "        dropout=0.1,\n",
        "        PAD_IDX=10,\n",
        "        activation=\"gelu\"\n",
        "    )\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(**trainer_flags)\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doVQv8eGiSSw"
      },
      "source": [
        "# TEST FEEDBACK-TRANSFORMER SEQ2SEQ MODEL\n",
        "\n",
        "trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbQDMDgfia0X"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk4B0rl6gvhw"
      },
      "source": [
        "# Algorithmic Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwQ9yxgwLnCu"
      },
      "source": [
        "# class AlgorithmicTaskDataset(Dataset):\n",
        "#     def __init__(self, num_samples=10000, max_length=100, reverse=False):\n",
        "\n",
        "#         self.sequence_pairs = self.generate_samples(num_samples, max_length, reverse)\n",
        "\n",
        "#     def generate_samples(self, num_samples=10000, max_length=100, reverse=False):\n",
        "#         sequence_pairs = []\n",
        "#         fns = [self.assign_fn, self.increment_fn, self.print_fn, self.conditional_fn]\n",
        "#         for i in range(num_samples):\n",
        "#             input_text, python_code = \"\", \"\"\n",
        "#         return sequence_pairs\n",
        "\n",
        "#     def assign_fn(self, var_string, val_string):\n",
        "#         input_string = str(\"{} = {} ;\".format(var_string, val_string))\n",
        "#         python_string = str(\"{} = {} ;\".format(var_string, val_string))\n",
        "#         return input_string, python_string\n",
        "\n",
        "#     def increment_fn(self, var_string, decrement=False):\n",
        "#         if decrement:\n",
        "#             input_string = str(\"{} -- ;\".format(var_string))\n",
        "#             python_string = str(\"{} -= 1 ;\".format(var_string))\n",
        "#         else:\n",
        "#             input_string = str(\"{} ++ ;\".format(var_string))\n",
        "#             python_string = str(\"{} += 1 ;\".format(var_string))\n",
        "#         return input_string, python_string\n",
        "\n",
        "#     def print_fn(self, var_string):\n",
        "#         input_string = str(\"print {} ;\".format(var_string))\n",
        "#         python_string = str(\"print({}) ;\".format(var_string))\n",
        "#         return input_string, python_string\n",
        "\n",
        "#     def conditional_fn(self, var_string, condition_string, comparator_string):\n",
        "#         input_string = str(\n",
        "#             \"if {} {} {} : \".format(var_string, condition_string, comparator_string)\n",
        "#         )\n",
        "#         python_string = str(\n",
        "#             \"if ({} {} {}): \".format(var_string, condition_string, comparator_string)\n",
        "#         )\n",
        "#         return input_string, python_string\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sequence_pairs)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         if torch.is_tensor(idx):\n",
        "#             idx = idx.tolist()\n",
        "#         sample = (\n",
        "#             torch.tensor(self.sequence_pairs[idx][\"input\"]),\n",
        "#             torch.tensor(self.sequence_pairs[idx][\"output\"]),\n",
        "#         )\n",
        "#         return sample"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}